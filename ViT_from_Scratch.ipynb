{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnxPrhbpNxOXLf8JKF84l1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jainam051/Vision-Transformer-from-scratch/blob/main/ViT_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiBstfVEB-k8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import einops\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, RandomHorizontalFlip, RandomCrop\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "\n"
      ],
      "metadata": {
        "id": "t8S79pAcIVec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "patch_size = 16\n",
        "latent_size = 768\n",
        "n_channels = 3\n",
        "num_heads = 12\n",
        "num_encoders = 12\n",
        "dropout = 0.1\n",
        "num_classes = 10\n",
        "size = 224\n",
        "\n",
        "epochs = 10\n",
        "base_lr = 10e-3\n",
        "weight_decay = 0.03\n",
        "batch_size = 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXKLq5oLIXjK",
        "outputId": "4013ebcd-434d-4378-e0a5-d044cbe5a817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "    def __init__(self, patch_size=patch_size, n_channels=n_channels, device=device, latent_size=latent_size, batch_size=batch_size):\n",
        "        super(InputEmbedding, self).__init__()\n",
        "        self.latent_size = latent_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_channels = n_channels\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.input_size = self.patch_size*self.patch_size*self.n_channels\n",
        "\n",
        "        # Linear projection\n",
        "        self.linearProjection = nn.Linear(self.input_size, self.latent_size)\n",
        "\n",
        "        # Class token\n",
        "        self.class_token = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size)).to(self.device)\n",
        "\n",
        "        # Positional embedding\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size)).to(self.device)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        input_data = input_data.to(self.device)\n",
        "\n",
        "        # Patchify input image\n",
        "        patches = einops.rearrange(\n",
        "            input_data, 'b c (h h1) (w w1) -> b (h w) (h1 w1 c)', h1=self.patch_size, w1=self.patch_size)\n",
        "\n",
        "        #print(input_data.size())\n",
        "        #print(patches.size())\n",
        "\n",
        "        linear_projection = self.linearProjection(patches).to(self.device)\n",
        "        b, n, _ = linear_projection.shape\n",
        "\n",
        "        linear_projection = torch.cat((self.class_token, linear_projection), dim=1)\n",
        "        pos_embed = einops.repeat(self.pos_embedding, 'b 1 d -> b m d', m=n+1)\n",
        "\n",
        "        linear_projection += pos_embed\n",
        "\n",
        "        return linear_projection\n"
      ],
      "metadata": {
        "id": "4efpKFZ4IZuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "test_input = torch.randn((8, 3, 224, 224))\n",
        "test_class = InputEmbedding().to(device)\n",
        "embed_test = test_class(test_input)\n",
        "\n"
      ],
      "metadata": {
        "id": "K4l4Y2EDIeKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, latent_size, num_heads, dropout=0.1):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert latent_size % num_heads == 0, \"latent_size must be divisible by num_heads\"\n",
        "\n",
        "        self.latent_size = latent_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = latent_size // num_heads\n",
        "\n",
        "        # Learnable projections\n",
        "        self.q_proj = nn.Linear(latent_size, latent_size)\n",
        "        self.k_proj = nn.Linear(latent_size, latent_size)\n",
        "        self.v_proj = nn.Linear(latent_size, latent_size)\n",
        "        self.out_proj = nn.Linear(latent_size, latent_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = math.sqrt(self.head_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, latent_size)\n",
        "        B, N, _ = x.shape\n",
        "\n",
        "        # Linear projections: (B, N, latent_size) -> (B, N, latent_size)\n",
        "        Q = self.q_proj(x)\n",
        "        K = self.k_proj(x)\n",
        "        V = self.v_proj(x)\n",
        "\n",
        "        # Reshape into heads: (B, N, latent_size) -> (B, num_heads, N, head_dim)\n",
        "        Q = einops.rearrange(Q, 'b n (h d) -> b h n d', h=self.num_heads)\n",
        "        K = einops.rearrange(K, 'b n (h d) -> b h n d', h=self.num_heads)\n",
        "        V = einops.rearrange(V, 'b n (h d) -> b h n d', h=self.num_heads)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # (B, H, N, N)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        context = torch.matmul(attn, V)  # (B, H, N, D)\n",
        "        context = einops.rearrange(context, 'b h n d -> b n (h d)')  # Concatenate heads\n",
        "\n",
        "        return self.out_proj(context)  # (B, N, latent_size)"
      ],
      "metadata": {
        "id": "vmRVhjwD3Dms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, latent_size=latent_size, num_heads=num_heads, dropout=dropout):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(latent_size)\n",
        "        self.attn = MultiHeadSelfAttention(latent_size, num_heads, dropout)\n",
        "        self.norm2 = nn.LayerNorm(latent_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(latent_size, latent_size * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(latent_size * 4, latent_size),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention + residual\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        # Feedforward + residual\n",
        "        x = x + self.feed_forward(self.norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "HwVCSFQaIieu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encoder = EncoderBlock().to(device)\n",
        "test_encoder(embed_test)"
      ],
      "metadata": {
        "id": "s6BoD38_IjL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Vit(nn.Module):\n",
        "    def __init__(self, num_encoders=num_encoders, latent_size=latent_size, device=device, num_classes=num_classes, dropout=dropout):\n",
        "        super(Vit, self).__init__()\n",
        "        self.num_encoder = num_encoders\n",
        "        self.latent_size = latent_size\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = InputEmbedding()\n",
        "\n",
        "        # Create the stack of encoders\n",
        "        self.encStack = nn.ModuleList([EncoderBlock() for i in range(self.num_encoder)])\n",
        "\n",
        "        self.MLP_head = nn.Sequential(\n",
        "            nn.LayerNorm(self.latent_size),\n",
        "            nn.Linear(self.latent_size, self.latent_size),\n",
        "            nn.Linear(self.latent_size, self.num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, test_input):\n",
        "        enc_output = self.embedding(test_input)\n",
        "\n",
        "        for enc_layer in self.encStack:\n",
        "            enc_output = enc_layer(enc_output)\n",
        "\n",
        "        cls_token_embed = enc_output[:, 0]\n",
        "\n",
        "        return self.MLP_head(cls_token_embed)\n",
        "\n"
      ],
      "metadata": {
        "id": "USRzayLTIn6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = Vit().to(device)\n",
        "vit_output = model(test_input)\n",
        "print(vit_output)\n",
        "print(vit_output.size())\n",
        "\n"
      ],
      "metadata": {
        "id": "1BjhALHVIqDt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}